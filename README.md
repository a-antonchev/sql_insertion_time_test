# Тестирование времени вставки данных в БД

Тестировались различные инструменты вставки данных в базы данных SQLite, PostgreSQL, ClickHouse.
Тест не претендует на истину в последней инстанции, при других условиях результаты могут быть другими.

## SQLite

Для тестирования использовались функции `execute` и `executemany` библиотеки
sqlite3. БД располагалась на локальном SSD-диске, при вставке 1_000_000 записей
из функции-генератора были получены результаты (см. блокнот`sqllite/test_insert_to_sqlite.ipynb`):

- `execute` в цикле: 3.7824 секунд
- `executemany`: 2.7647 секунд

Функция `executemany` может быстрее выполняться по сравнению с `execute` за счет того,
что она компилирует запрос только один раз и затем выполняет его несколько раз с
разными параметрами. В случае с `execute`, каждый вызов требует отдельной компиляции
запроса, что является более медленным процессом.

## PostgreSQL

В качестве сервера использовался docker-контейнер, поднятый на HDD-диске на удаленной
машине в локальной 1G сети (см. Dockerfile и команды в `postgresql/`).

Для тестирования использовались функции из библиотеки psycopg2 и один вариант вставки
из датафрейма pandas с использованием `DataFrame.to_sql` на движке sqlalchemy.
Были получены результаты (см. блокнот 'postgresql/test_insert_to_postgres.ipynb'):

**1. psycopg2**

- `Cursor.execute`: в цикле при вставке из генератора: 246.3413 секунд
- `Cursor.executemany`: при вставке из генератора: 240.3837 секунд
- `extras.execute_values` при вставке из генератора: 14.0147 секунд
- `Cursor.copy_from` при вставке из CSV-файла: 1.5685 секунд

**2. sqlalchemy + pandas DataFrame.to_sql**

- pandas `DataFrame.to_sql`: 40.1986 секунд

Явные аутсайдеры 'execute' и `executemany`, при их использовании, каждая строка данных
передается в PostgreSQL как отдельный запрос, что приводит к дополнительным накладным
расходам на обработку запросов и передачу данных.
Результаты у `execute` и `executemany` практически не отличаются, об этом есть
предупреждение на странице официальной документации:

> Warning In its current implementation this method is not faster than executing
> execute() in a loop.

Согласно документации psycopg2, `executemany` является просто оберткой над `execute`
и фактически использует `execute` под капотом.

Отличный результат у `execute_values`, функция позволяет передавать данные в виде
списка кортежей, который затем передается в PostgreSQL как единый запрос.
Важным является параметр `page_size` (по умолчанию =100). Его увеличение позволяет
добиться улучшения производительности при вставке большого объема данных.

> Psycopg will join the statements into fewer multi-statement commands, each one
> containing at most *page_size* statements, resulting in a reduced number of server
> roundtrips

Функция `copy_from` показывает наилучшую производительность по сравнению с другими
методами вставки данных за счет использования механизма копирования данных в PostgreSQL - 
она использует команду COPY PostgreSQL.

Вариант sqlalchemy + pandas DataFrame.to_sql показал достаточно низкую производительность
(порядка 40 секунд), изменение размера `chunksise` (1000, 10_000, 100_000, 500_000)
не привело к сколько нибудь значимому изменению времени вставки, а добавление
параметра `method='multi'` привело к увеличению времени выполнения до ~100 секунд.
Другие способы вставки из sqlalchemy не тестировались.

## ClickHouse

В качестве сервера использовался docker-контейнер, поднятый на HDD-диске на удаленной
машине в локальной 1G сети (см. Dockerfile и команды в `clickhouse/`).

Для тестирования использовались функции из пакетов `clickhouse_driver` (работает через
нативный протокол, стандартный порт 9000) и `clickhouse_connect` (работает через HTTP,
стандартный порт 8123).

Были получены результаты (см. блокнот 'clickhouse/test_insert_to_clickhouse.ipynb'):

**1. clickhouse_driver**

1.1. DB API 2.0:
- `Cursor.execute`: замеры не проводились, метод катастрофически медленный при большом количестве записей
- `Cursor.executemany` при вставке из списка: 0.9053 секунд
- `Cursor.executemany` при вставке из генератора: 1.6330 секунд
- `Cursor.executemany` при вставке из CSV-генератора: 6.4458 секунд

1.2. API:
- `Client.execute` при вставке из списка: 0.8542 секунд
- `Client.execute` при вставке из генератора: 1.6399 секунд
- `Client.execute` при вставке из CSV-генератора: 6.2741 секунд
- `Client.insert_dataframe` при вставке из DataFrame: 0.9247 секунд

`Cursor.execute` показал абсолютно неудовлетворительные результаты при вставке большого
количества данных, на вставке 1 млн.записей не тестировался.

`Cursor.executemany` показал отличные результаты при вставке из подготовленного списка,
применение этой же функции при вставке из генератора привело к падению производительности
на ~75% (тут определенный компромисс между производительностью и объемом используемой
памяти для организации списка).
Вставка из CSV-генератора привела к увеличению времени в ~6 раз по сравнению
со вставкой из списка.

`Client.execute` (из собственного API библиотеки) показал аналогичные результаты, так
как, скорее всего, "под капотом" использует Cursor.

Функция `Client.insert_dataframe` показала результат, сравнимый с Cursor.execute при
вставке из списка, скорее всего, функция не использует Cursor, но оптимизирована для
вставки больших объемов данных в таблицу, используя блоковую структуру ClickHouse.

**2. clickhouse_connect**

- `Client.insert` при вставке из списка: 2.9978 секунд
- `Client.insert_df` при вставке из DataFrame: 1.1556 секунд
- `insert_file` при вставке из файла: 0.4547 секунд

В этой библиотеке функция insert_file показала рекордное время вставки данных, видимо она
специально оптимизирована для работы с большими объемами (буферизация? параллельная вставка?).
